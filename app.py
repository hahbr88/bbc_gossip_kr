from __future__ import annotations

import os
import re
import time
from datetime import datetime
from zoneinfo import ZoneInfo
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
from bs4 import BeautifulSoup
from deep_translator import GoogleTranslator

# ë¡œì»¬ì—ì„œë§Œ .env ë¡œë“œ (Lambdaì—ì„œëŠ” ìë™ìœ¼ë¡œ í™˜ê²½ë³€ìˆ˜ ì£¼ì…ë¨)
if os.getenv("AWS_LAMBDA_FUNCTION_NAME") is None:
    try:
        from dotenv import load_dotenv
        load_dotenv()
    except ModuleNotFoundError:
        # python-dotenvê°€ ì—†ì–´ë„ ë¡œì»¬ì—ì„œ exportë¡œ ì‹¤í–‰ ê°€ëŠ¥í•˜ê²Œ
        pass

HEADERS = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
GOSSIP_MAIN_URL = "https://www.bbc.com/sport/football/gossip"
ARTICLE_SELECTOR = "div[data-component='text-block'] p[class*='Paragraph']"

DRY_RUN = os.getenv("DRY_RUN") == "1"


def get_config() -> str:
    slack_webhook = os.getenv("SLACK_WEBHOOK_URL")
    if not slack_webhook:
        raise ValueError("SLACK_WEBHOOK_URL í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
    return slack_webhook


def make_session() -> requests.Session:
    retry = Retry(
        total=2,
        connect=2,
        read=2,
        status=2,
        backoff_factor=0.25,
        status_forcelist=(429, 500, 502, 503, 504),
        allowed_methods=("GET", "POST"),
        raise_on_status=False,
        respect_retry_after_header=True,
    )
    adapter = HTTPAdapter(max_retries=retry, pool_connections=10, pool_maxsize=10)
    s = requests.Session()
    s.mount("https://", adapter)
    s.mount("http://", adapter)
    return s


SESSION = make_session()


def fetch_html(url: str) -> BeautifulSoup:
    res = SESSION.get(url, headers=HEADERS, timeout=(3, 10))
    res.raise_for_status()
    return BeautifulSoup(res.text, "html.parser")


def send_slack_message(text: str, webhook_url: str) -> None:
    if DRY_RUN:
        print("[DRY_RUN] Slack ì „ì†¡ ìƒëµ. ë¯¸ë¦¬ë³´ê¸°(ì• 500ì):\n", text[:500])
        return
    res = SESSION.post(webhook_url, json={"text": text}, timeout=(3, 10))
    res.raise_for_status()


def get_latest_gossip_url() -> str | None:
    soup = fetch_html(GOSSIP_MAIN_URL)
    article = soup.select_one("a[href*='/sport/football/articles/']")
    if not article:
        return None
    return "https://www.bbc.com" + article["href"]


def parse_gossip_article(url: str) -> tuple[str, str | None, BeautifulSoup]:
    soup = fetch_html(url)
    h1 = soup.find("h1")
    title = h1.get_text(strip=True) if h1 else "BBC Football Gossip"

    time_tag = soup.find("time")
    published_datetime = (
        time_tag["datetime"] if time_tag and time_tag.has_attr("datetime") else None
    )

    return title, published_datetime, soup


def clean_gossip_text(text: str) -> str:
    text = re.sub(r"\s*,?\s*external\b", "", text, flags=re.IGNORECASE)
    text = re.sub(r"\s+'s", "'s", text)
    text = re.sub(r"\s+\.", ".", text)
    text = re.sub(r"\s+,", ",", text)
    text = re.sub(r"\(\s+", "(", text)
    text = re.sub(r"\s+\)", ")", text)
    text = re.sub(r"\s{2,}", " ", text)
    text = text.replace("Ã‚Â£", "Â£")
    return text.strip()


def extract_gossip_items(soup: BeautifulSoup) -> list[str]:
    items: list[str] = []
    for p in soup.select(ARTICLE_SELECTOR):
        raw = p.get_text(" ", strip=True)
        if len(raw) < 50:
            continue
        items.append(clean_gossip_text(raw))
    return items


def is_today_article(published_datetime_str: str | None) -> bool:
    if not published_datetime_str:
        return False

    published_utc = datetime.fromisoformat(published_datetime_str.replace("Z", "+00:00"))
    published_kst = published_utc.astimezone(ZoneInfo("Asia/Seoul"))
    today_kst = datetime.now(ZoneInfo("Asia/Seoul")).date()
    return published_kst.date() == today_kst

SOURCE_PATTERN = re.compile(r"\s*\(([^)]+)\)\s*$")
def split_source_tail(text: str) -> tuple[str, str]:
    """
    ë¬¸ì¥ ë§¨ ëì˜ (ì¶œì²˜) ê¼¬ë¦¬ë¥¼ ë¶„ë¦¬í•œë‹¤.
    - ë°˜í™˜: (ë³¸ë¬¸, ì¶œì²˜ê¼¬ë¦¬)  ì˜ˆ) ("... season.", " (Mirror)")
    """
    m = SOURCE_PATTERN.search(text)
    if not m:
        return text.strip(), ""
    main = text[:m.start()].strip()
    tail = m.group(0)  # ê´„í˜¸ í¬í•¨ ì›ë¬¸ ê·¸ëŒ€ë¡œ
    return main, tail

def make_token(i: int) -> str:
    # ë²ˆì—­ê¸°ê°€ ê±´ë“œë¦¬ê¸° ì–´ë ¤ìš´ í† í° í˜•íƒœ(ëŒ€ë¬¸ì/êº¾ì‡ )
    return f"<<<SRC{i}>>>"

def run() -> dict:
    print("ğŸš€ BBC Gossip ì‹¤í–‰")

    try:
        webhook_url = get_config()
    except ValueError as e:
        return {"statusCode": 500, "body": str(e)}

    t0 = time.perf_counter()
    url = get_latest_gossip_url()
    print("t(get_latest):", time.perf_counter() - t0)
    if not url:
        return {"statusCode": 404, "body": "ê¸°ì‚¬ ë§í¬ ëª» ì°¾ìŒ"}

    t1 = time.perf_counter()
    title, published_date, soup = parse_gossip_article(url)
    print("t(parse):", time.perf_counter() - t1)
    if not is_today_article(published_date):
        return {"statusCode": 200, "body": "ì˜¤ëŠ˜ ê¸°ì‚¬ ì•„ë‹˜"}

    items = extract_gossip_items(soup)
    if not items:
        return {"statusCode": 200, "body": "ê°€ì‹­ ì—†ìŒ"}

    

    # ---- ì—¬ê¸°ë¶€í„° ì†ë„ ìš°ì„  ë²ˆì—­ ë¡œì§ ----
    tails: list[str] = []
    lines: list[str] = []

    for i, x in enumerate(items):
        main, tail = split_source_tail(x)
        token = make_token(i)
        tails.append(tail)
        lines.append(f"â€¢ {main} {token}")  # ë³¸ë¬¸ ë’¤ì— í† í°

    refined_with_tokens = "\n".join(lines)

    translator = GoogleTranslator(source="en", target="ko")
    
    t2 = time.perf_counter()
    try:
        translated = translator.translate(refined_with_tokens)
    except Exception as e:
        print("âŒ ë²ˆì—­ 1ì°¨ ì‹¤íŒ¨(ì¬ì‹œë„) :", e)
        try:
            translated = translator.translate(refined_with_tokens)
        except Exception as e2:
            print("âŒ ë²ˆì—­ 2ì°¨ ì‹¤íŒ¨ (ì›ë¬¸ì„ ê·¸ëŒ€ë¡œ ë°˜í™˜):", e2)
            translated = refined_with_tokens

    print("t(translate_once):", time.perf_counter() - t2)

    def format_source_tail(tail: str) -> str:
        if not tail:
            return "\n"
        return f"\n*{tail.strip()}*\n"
    

    for i, tail in enumerate(tails):
        translated = translated.replace(make_token(i), format_source_tail(tail))

    # ë²ˆì—­ ê²°ê³¼ì— í† í°ì„ ì¶œì²˜ ê¼¬ë¦¬ë¡œ ë˜ëŒë¦¬ê¸°
    for i, tail in enumerate(tails):
        translated = translated.replace(make_token(i), tail)

    # í† í° ë³µì›ì´ ëˆ„ë½ëœ ê²½ìš°(ë²ˆì—­ê¸°ê°€ í† í°ì„ í›¼ì†í•œ ì¼€ì´ìŠ¤) ì•ˆì „ ì²˜ë¦¬
    if "<<<SRC" in translated:
        print("âš ï¸ ì¼ë¶€ ì¶œì²˜ í† í°ì´ ë³µì›ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. (ë²ˆì—­ê¸°ê°€ í† í°ì„ ë³€ê²½í–ˆì„ ìˆ˜ ìˆìŒ)")

    message = f"*{title}*\n\n{translated}"

    t3 = time.perf_counter()
    send_slack_message(message, webhook_url)
    print("t(slack):", time.perf_counter() - t3)

    return {"statusCode": 200, "body": f"Gossip {len(items)}ê°œ"}


if __name__ == "__main__":
    # ë¡œì»¬ ì‹¤í–‰ìš©
    result = run()
    print("result:", result)
